"""
Stereo Photogrammetry technique data definition.
"""

# Data for Stereo Photogrammetry reference page
stereo_photogrammetry_data = {'one_line_summary': 'Stereo photogrammetry, often implemented using Structure-from-Motion (SfM) algorithms combined with Multi-View Stereo (MVS) reconstruction, is a technique for generating accurate three-dimensional models of objects and scenes from overlapping two-dimensional photographs. The method exploits parallax - the apparent displacement of features when viewed from different positions - to triangulate 3D coordinates. Modern photogrammetric workflows automatically identify corresponding features across multiple images, estimate camera positions and orientations, and reconstruct dense 3D point clouds and textured meshes with millimeter to sub-millimeter accuracy. In cultural heritage, photogrammetry enables comprehensive digital documentation of artifacts, monuments, archaeological sites, and collections, producing metrologically accurate 3D models for analysis, monitoring, conservation planning, virtual exhibitions, and public engagement.', 'abstract': 'Stereo photogrammetry, often implemented using Structure-from-Motion (SfM) algorithms combined with Multi-View Stereo (MVS) reconstruction, is a technique for generating accurate three-dimensional models of objects and scenes from overlapping two-dimensional photographs. The method exploits parallax - the apparent displacement of features when viewed from different positions - to triangulate 3D coordinates. Modern photogrammetric workflows automatically identify corresponding features across multiple images, estimate camera positions and orientations, and reconstruct dense 3D point clouds and textured meshes with millimeter to sub-millimeter accuracy. In cultural heritage, photogrammetry enables comprehensive digital documentation of artifacts, monuments, archaeological sites, and collections, producing metrologically accurate 3D models for analysis, monitoring, conservation planning, virtual exhibitions, and public engagement.', 'physics_principle': 'Stereo photogrammetry reconstructs 3D geometry from 2D images by exploiting geometric constraints of perspective projection. Each photograph is a perspective projection: 3D point P(X,Y,Z) in world coordinates projects to 2D point p(x,y) in image plane according to pinhole camera model. Same physical point appears at different image coordinates in photos taken from different positions (parallax). By identifying corresponding features (same physical point) across multiple images and knowing camera parameters (focal length, position, orientation), 3D coordinates can be triangulated via ray intersection. Modern SfM automatically solves for both 3D structure and camera parameters simultaneously: (1) Feature detection (SIFT, SURF): identify distinctive image features. (2) Feature matching: find correspondences across images. (3) Bundle adjustment: optimize camera poses and 3D points to minimize reprojection error. (4) Dense reconstruction (MVS): compute depth for every pixel to create dense point cloud.', 'instruments_components': '**Source:**\nAmbient illumination or supplemental lighting (flash, LED panels, strobes). Natural daylight, museum lighting, or controlled artificial light; no active projection required (passive photogrammetry). Visible light spectrum (400-700 nm). Ambient to ~1000 lux illumination (sufficient for typical camera exposures). Diffuse, uniform illumination preferred; avoid harsh shadows and specular highlights.\n\n**Detector:**\nDigital camera: DSLR, mirrorless, smartphone. Sensor: CMOS or CCD. Color: RGB (Bayer pattern) typical. Visible light (400-700 nm); some cameras extended to near-IR (up to 1000 nm). Quantum efficiency 40-80% (modern CMOS sensors). Time resolution: Frame rate: single exposures (still photography) typically; video-rate photogrammetry possible (30-120 fps).\n\n**Critical Components:**\n- High-resolution camera (20-60 MP recommended for high-detail work)\n- Quality lens (low distortion, sharp across field; prime lenses preferred over zoom)\n- Stable camera position (tripod or careful handheld technique to avoid motion blur)\n- Scale reference (ruler, coded targets, or known dimensions for metric reconstruction)\n- Sufficient image overlap (60-80% between adjacent images for reliable matching)\n- Processing computer (multi-core CPU, 16-64 GB RAM, GPU with 4-8 GB VRAM for dense reconstruction)\n- Photogrammetry software (Agisoft Metashape, RealityCapture, Meshroom, etc.)\n\n**Typical Configuration:**\n**Standard close-range photogrammetry setup:** Full-frame DSLR (24-45 MP, e.g., Nikon D850, Canon EOS R5). Prime lens 50 mm f/1.8 or 35 mm f/2.8 (low distortion, sharp). Tripod-mounted (for stability) or careful handheld. Camera settings: aperture f/8-f/11 (deep depth of field), ISO 100-400 (low noise), shutter speed to avoid motion blur (1/60 s or faster). Capture 30-200 images with 70% overlap for small object (0.5 m) to large monument (10 m). Include scale bar (calibrated length, e.g., 0.5 m or 1 m) in several images. Process on workstation: Intel i7/i9 or AMD Ryzen 9, 32 GB RAM, NVIDIA RTX 3060 or better GPU. Software: Agisoft Metashape Professional. Processing time: 1-6 hours for typical object (50 images, 20 MP, high quality).', 'resolution_detection': '**Spatial Resolution:**\nLateral Resolution: 0.01-10 mm (depends on GSD: higher resolution camera, closer distance, longer focal length = finer detail). Depth Resolution: 0.1-50 mm (typically 1:100,000 to 1:1000 of object distance; depends on baseline and pixel precision). Limiting factors: Camera resolution (more megapixels = finer detail; 20-60 MP cameras common), Distance to object (closer = better resolution; limited by depth of field), Focal length (longer lens = higher magnification but narrower field of view), Baseline between images (larger baseline = better depth resolution but requires more overlap), Image quality (blur, noise, compression artifacts reduce feature localization accuracy), Textureless regions (uniform surfaces lack distinctive features for matching).\n\n**Interaction Depth:**\nPhotogrammetry is a pure surface technique, measuring only visible surfaces. Penetration depth is zero - the method reconstructs the external 3D geometry based on what the camera can see. Occluded or internal structures are not captured unless additional views provide line-of-sight access. Opaque materials (stone, metal, wood): Surface only, ideal for photogrammetry; well-defined visible surface. Transparent materials (glass, crystal): Surface ambiguous, challenging - refraction distorts geometry; reflections create artifacts. Highly reflective surfaces (polished metal, glazed ceramics): Surface only, specular highlights cause correspondence failures; may need coating or cross-polarization. Dark or textureless surfaces: Surface only, low contrast → fewer features → reduced accuracy; may need projected texture. Complex geometry (filigree, hair, fine detail): Visible surfaces only, self-occlusion limits completeness; requires many views from multiple angles.\n\n**Detection Limits:**\nMinimum feature size: ~2-3 pixels in image. Surface detail: ~0.1-1 mm for close-range (0.5-2 m) with DSLR. Dynamic range: Depth range: limited by depth of field (everything in focus required). Typical: 1:10 to 1:100 (object depth : camera distance). Signal-to-noise: Accuracy: 1:50,000 typical for well-controlled close-range; 1:1,000 for casual smartphone capture.', 'sample_requirements': '**Destructiveness:** non-destructive\n\n**Portability:** highly-portable\n\n**Sample Preparation:**\n[\'Sampling required: No - no size constraints (millimeter-scale artifacts to kilometer-scale monuments)\', \'Position object stably. If possible, place on turntable for controlled rotation\', "Ensure object won\'t move during capture session", \'For outdoor monuments, no positioning needed\', \'No preparation required for most materials\', \'For challenging surfaces: (1) Highly reflective/shiny: apply dulling spray (removable) or cross-polarized lighting, (2) Transparent: coat with talcum powder or developer spray (carefully, test on inconspicuous area first), (3) Very dark: supplement lighting, (4) Featureless: project random texture pattern (for industrial objects; generally avoided in heritage)\', \'Not applicable - photogrammetry measures external geometry only\', \'Surface coatings (dulling spray, powder) alter appearance; remove carefully post-capture\', \'Movement during capture (wind, vibration) creates misalignment and blur\', \'Changing lighting (sun moving, clouds, shadows) alters appearance between images\', \'Dust or particles on object appear as surface features; clean gently if possible\']', 'measurement_protocol': {'preparation': ['[\'Sampling required: No - no size constraints (millimeter-scale artifacts to kilometer-scale monuments)\', \'Position object stably. If possible, place on turntable for controlled rotation\', "Ensure object won\'t move during capture session", \'For outdoor monuments, no positioning needed\', \'No preparation required for most materials\', \'For challenging surfaces: (1) Highly reflective/shiny: apply dulling spray (removable) or cross-polarized lighting, (2) Transparent: coat with talcum powder or developer spray (carefully, test on inconspicuous area first), (3) Very dark: supplement lighting, (4) Featureless: project random texture pattern (for industrial objects; generally avoided in heritage)\', \'Not applicable - photogrammetry measures external geometry only\', \'Surface coatings (dulling spray, powder) alter appearance; remove carefully post-capture\', \'Movement during capture (wind, vibration) creates misalignment and blur\', \'Changing lighting (sun moving, clouds, shadows) alters appearance between images\', \'Dust or particles on object appear as surface features; clean gently if possible\']'], 'data_collection': ["**STEP 1: Planning and Setup (10-60 minutes)**\n- Assess object or site: size, geometry, accessibility, lighting\n- Plan camera positions: ensure complete coverage (every part visible from ≥2 views), maintain ~70% overlap between adjacent images\n- Set up lighting if indoors (diffuse, even illumination; avoid harsh shadows)\n- Place scale bars or coded targets in scene (for metric accuracy and quality control)\n- Position color chart if color fidelity critical\n- Critical parameters: Adequate lighting (avoid shadows, highlights), Scale reference visible in multiple images, Stable object (won't move during capture)\n\n**STEP 2: Camera Configuration (5-10 minutes)**\n- Set camera to manual mode for consistency\n- Aperture: f/8-f/11 (balance depth of field and sharpness; avoid diffraction at f/16+)\n- ISO: as low as possible (100-400) to minimize noise\n- Shutter speed: fast enough to avoid motion blur (1/60 s minimum handheld; 1/125 s+ safer; use tripod for longer exposures)\n- Focus: manual focus on object, lock focus (avoid auto-focus hunting between shots)\n- Image format: RAW preferred (maximum dynamic range, no compression artifacts) or highest-quality JPEG if RAW unavailable\n- Disable image stabilization if on tripod\n- Critical parameters: Consistent exposure across all images, Sharp focus throughout, No motion blur, RAW format if possible\n\n**STEP 3: Test Shots and Validation (5-10 minutes)**\n- Capture 3-5 test images from different positions\n- Review on camera LCD: check focus (zoom to 100%, inspect sharpness), check exposure (histogram - avoid clipping highlights/shadows), check coverage (features overlap between views)\n- Adjust camera settings if needed\n- Verify scale bar visible and sharp in test images\n- Critical parameters: All images sharp and well-exposed, Overlap confirmed, Scale visible\n\n**STEP 4: Systematic Image Acquisition (10 minutes to several hours)**\n- Capture images systematically: (1) For objects: circular pattern around object (every 10-20°), multiple heights (top, middle, bottom), close-ups of complex details, (2) For sites/monuments: grid pattern, parallel rows with 70% overlap, oblique views for vertical surfaces\n- Maintain consistent distance (for uniform GSD)\n- Shoot 50-200+ images depending on size and complexity\n- Include overall context shots (wide views showing object in surroundings)\n- Ensure every surface visible from at least 2 views (3+ preferred)\n- Critical parameters: 70-80% overlap between adjacent images, Every surface captured from ≥2 angles, Consistent camera settings throughout, No blurred images\n\n**STEP 5: Redundancy and Quality Checks (10-30 minutes)**\n- Review captured images on camera or laptop: check for blur, poor exposure, coverage gaps\n- Re-shoot any problematic images immediately (while setup still in place)\n- Add extra images in areas of geometric complexity (holes, undercuts, fine details)\n- Capture additional images with scale bar from multiple angles if metric accuracy critical\n- Critical parameters: No gaps in coverage, All images meet quality standards\n\n**STEP 6: Data Transfer and Backup (10-30 minutes)**\n- Transfer images to computer (USB, card reader)\n- Organize in project folder with clear naming (date, object ID)\n- Create backup copies on separate drive or cloud storage (RAW images valuable; processing can be redone, but original captures cannot)\n- Document capture conditions: camera/lens used, settings, date, lighting, any challenges\n\n**STEP 7: Image Preprocessing (Optional, 10-60 minutes if performed)**\n- If needed: (1) Convert RAW to TIFF or high-quality JPEG (preserve maximum information), (2) Adjust exposure, contrast, white balance for consistency (apply same adjustments to all images), (3) Mask out background (if cluttered, can confuse feature matching)\n- Generally, minimal preprocessing preferred - let software handle image variations\n\n**STEP 8: Software Processing - Alignment (10 minutes to 2 hours automated)**\n- Import images into photogrammetry software (Metashape, RealityCapture, Meshroom)\n- Feature detection and matching: software automatically detects features (SIFT, SURF), matches across images\n- Camera alignment (SfM): software estimates camera positions and sparse 3D point cloud\n- Bundle adjustment: refines camera parameters and 3D points to minimize reprojection error\n- Review sparse cloud: check coverage (all images aligned?), remove outliers, verify scale bar detected\n- Critical parameters: All images successfully aligned, Reprojection error <1 pixel, Sparse point cloud represents object geometry\n\n**STEP 9: Dense Reconstruction (30 minutes to 12 hours automated)**\n- Dense stereo matching (MVS): software computes depth for every pixel using multi-view constraints\n- Generates dense point cloud (millions to billions of points)\n- Settings: quality level (low/medium/high/ultra - higher = more detail but slower)\n- Review dense cloud: check completeness (gaps?), noise level, level of detail\n- Filter noise points if needed (statistical outlier removal)\n- Critical parameters: Dense cloud captures fine surface detail, Minimal noise and artifacts, Completeness adequate for application\n\n**STEP 10: Mesh Generation and Texturing (10 minutes to 2 hours automated)**\n- Mesh generation: convert point cloud to polygonal mesh (triangulated surface)\n- Settings: face count (higher = more detail but larger file), surface type (arbitrary for complex objects, height field for terrain)\n- Mesh editing: fill holes, smooth noisy regions, decimate (reduce polygon count for web viewing)\n- Texture mapping: project original photos onto mesh to create photo-realistic appearance\n- Review textured mesh: check for texture seams, color consistency, alignment\n- Critical parameters: Mesh accurately represents surface geometry, Texture well-aligned and color-consistent, File size manageable for intended use\n\n**STEP 11: Scaling and Georeferencing (10-30 minutes)**\n- If scale bar used: measure distance between markers in software, set to known real-world length\n- Software rescales entire model\n- Verify scale: measure known dimensions on model, compare to ground truth\n- If accuracy critical: use multiple scale bars or Ground Control Points (GCPs) with known coordinates\n- Check coordinate system (local vs. geographic)\n- Export scaled model\n- Critical parameters: Model scaled to correct real-world dimensions, Accuracy verified via independent measurements\n\n**STEP 12: Export and Deliverables (10-60 minutes)**\n- Export point cloud (formats: .xyz, .las, .ply, .e57)\n- Export mesh (formats: .obj, .ply, .stl, .fbx, .dae)\n- Export textures (JPEG, PNG, TIFF)\n- Generate orthophotos (2D metric images from nadir or oblique views)\n- Export metadata: camera positions, accuracy report, processing log\n- Optimize for web viewing if needed (Sketchfab, 3DHOP): decimate mesh, compress textures"], 'calibration': ["**Standards used:** Checkerboard or coded target calibration pattern (for camera intrinsics), Certified scale bars (known length with traceable uncertainty, e.g., 0.5 m ± 0.1 mm), Total station or laser scanner reference data (for accuracy validation), Certified test objects (e.g., NPL's photogrammetry artifact with known dimensions).\n\n**Frequency:** Camera calibration: once per camera-lens combination (or if lens settings changed, e.g., focus ring moved). Scale bar: include in every project for metric accuracy. Validation: periodic (monthly to yearly) for quality assurance.\n\n**Procedure:** Camera calibration: capture 20-40 images of checkerboard from various angles and distances. Software computes intrinsic parameters (focal length, principal point, radial/tangential distortion). Save calibration file for future projects. Scale bar: place in scene during capture, measure in software, set known length. Validation: compare photogrammetry-derived measurements to independent reference (ruler, calipers, laser scanner) on certified artifact."]}, 'data_outputs': '**Raw Data Format:**\n**File Formats:** Input: RAW (camera-specific: .NEF, .CR2, .ARW), TIFF, JPEG. Output: Point cloud (.xyz, .las, .ply, .e57), Mesh (.obj, .ply, .stl, .fbx), Images (JPEG, TIFF).\n\n**Data Structure:** Point cloud: N × 3 array (X, Y, Z coordinates) + optional RGB color. Mesh: vertices (N × 3), faces/triangles (M × 3 vertex indices), texture UV coordinates, texture image.\n\n**Typical File Sizes:** Images: 5-50 MB each (RAW); 50-200 images = 2.5-10 GB total. Dense point cloud: 100 MB - 10 GB (10⁶ - 10⁹ points). Textured mesh: 50 MB - 2 GB (depends on decimation and texture resolution).', 'data_analysis_pipeline': {'preprocessing': '- Image culling: Remove blurred, poorly exposed, or redundant images. Keep only high-quality images for processing. Reduces processing time and improves alignment reliability. Software: Manual review, FastStone Image Viewer, Adobe Bridge\n- White balance and color correction: Adjust white balance for color accuracy (use color chart if captured). Apply consistent adjustments to all images. Important for texture quality. Software: Adobe Lightroom, RawTherapee, darktable\n- Masking: Create alpha channel masks to exclude background, unwanted objects, or clutter. Helps feature matching focus on object of interest. Reduces noise and spurious geometry. Software: Photoshop, GIMP, Metashape (built-in masking tools)\n- Image resizing (optional): Downscale very high-resolution images (e.g., 60 MP → 20 MP) if full resolution unnecessary. Speeds processing. Generally not recommended for heritage (preserve maximum detail). Software: ImageMagick, batch processing scripts', 'analysis_workflow': '**STEP 1: Initial Alignment and Sparse Reconstruction**\n- Load images into photogrammetry software. Run feature detection and matching. Estimate camera positions (SfM). Perform bundle adjustment. Inspect sparse point cloud: verify all images aligned, check geometry looks reasonable, identify outliers. Generate alignment report: reprojection error (should be <1 pixel), number of tie points, camera positions.\n\n**STEP 2: Optimization and Gradual Selection**\n- Optimize camera parameters (refine intrinsics and extrinsics). Use gradual selection to remove low-quality tie points: (1) Reconstruction uncertainty (remove points with high uncertainty), (2) Reprojection error (remove points with error >2-3 pixels), (3) Projection accuracy (remove poorly localized points). Re-optimize after each removal. Iteratively improve alignment quality.\n\n**STEP 3: Scaling and Coordinate System Definition**\n- Create scale bars by identifying markers in images and setting known distance. Alternatively, place Ground Control Points (GCPs) with known coordinates for georeferencing. Update transformation: software scales model to real-world units and orients coordinate system. Verify accuracy: measure check distances, compare to ground truth.\n\n**STEP 4: Dense Point Cloud Generation**\n- Run dense reconstruction (MVS). Select quality level (high or ultra for heritage). Monitor processing (can take hours for large projects). Inspect dense cloud: zoom in to check detail level, identify noise or gaps. Export dense cloud if needed for analysis (CloudCompare, GIS software).\n\n**STEP 5: Mesh Generation**\n- Convert dense cloud to mesh. Choose surface type (arbitrary for complex 3D objects; height field for terrain). Set target face count (1-10 million typical for heritage; balance detail and file size). Inspect mesh: check for holes, flipped normals, artifacts. Edit if needed: fill small holes, smooth noisy areas.\n\n**STEP 6: Texture Generation**\n- Generate texture by projecting photos onto mesh. Settings: texture size (4096×4096 to 8192×8192 pixels typical), blending mode (mosaic for sharp boundaries, average for smooth blending), color correction (enable for consistent appearance). Inspect textured mesh: check for seams, color consistency, alignment. Re-generate with different settings if unsatisfactory.\n\n**STEP 7: Quantitative Analysis (Measurements, Comparisons)**\n- Perform measurements on 3D model: distances, areas, volumes, cross-sections. Compare multiple epochs (time-series photogrammetry) to detect changes: compute signed distance between meshes (change detection), generate difference maps (color-coded deformation). Extract orthophotos (metric 2D images) for documentation. Calculate surface parameters: roughness, curvature, slope.\n\n**STEP 8: Export and Optimization**\n- Export final deliverables: dense point cloud, textured mesh, orthophotos, processing report. Optimize mesh for specific uses: (1) Web viewing (Sketchfab): decimate to 50K-200K faces, compress textures to 2K-4K, (2) 3D printing: check manifold, repair holes, ensure correct scale and units, (3) GIS integration: export in appropriate coordinate system with metadata.'}, 'artifacts_troubleshooting': '**Troubleshooting:**\n- All images sharp (no motion blur, good focus)\n- Consistent exposure and white balance across image set\n- Sufficient overlap (70%+ between adjacent images)\n- All surfaces visible from at least 2 viewing angles\n- Camera alignment successful (all images registered)\n- Reprojection error <1 pixel (RMS; indicates good geometric consistency)\n- Dense point cloud complete (no large gaps or missing regions)\n- Model scaled correctly (measurements match ground truth within tolerance)\n- Texture well-aligned (no seams or color banding)\n- Common artifacts: Doming effect - curved surface appears on flat object due to insufficient camera network geometry (parallel camera positions), Bowling effect - similar to doming, systematic depth error from poor geometry, Noise and outliers - spurious points away from surface (from matching errors, reflections, moving objects), Holes and gaps - missing geometry due to occlusion, specular highlights, or low texture, Texture seams - visible boundaries where different photos meet on mesh (from lighting/exposure differences), Scale error - model dimensions incorrect (scale bar not measured or used incorrectly), Misalignments - chunks of model not properly registered (insufficient overlap or dynamic scene)\n- Troubleshooting: Some images not aligned (excluded from reconstruction) (check image quality (sharp, well-exposed); ensure overlap >60%; add more images to connect disconnected chunks; disable problematic images and re-align), High reprojection error (>1-2 pixels RMS) (use sharper images; calibrate camera properly; use global shutter camera if available; optimize camera parameters in bundle adjustment; remove outlier points), Large gaps or holes in reconstruction (capture additional views to see occluded areas; cross-polarize to reduce reflections; add temporary texture (careful: removable); accept limitation (some surfaces inherently difficult)), Doming or bowling (systematic depth error) (increase baseline diversity: capture from varying distances, heights, angles; add oblique and orthogonal views; avoid perfectly circular capture pattern; use more convergent geometry), Noisy point cloud (many spurious outliers) (improve image quality (sharper, lower ISO); capture under consistent lighting; remove moving objects from scene; apply noise filtering (statistical outlier removal, radius filtering) in post-processing), Incorrect scale (model too large or small) (re-measure scale bar carefully; verify markers clearly identified; use multiple scale bars for redundancy; cross-check dimensions against known measurements)', 'multimodal_pairings': '**Complementary Techniques:**\n\n- laser-scanning\n\n- raman-microscopy\n\n- xrf\n\n- thermography\n\n- rti', 'strengths_limitations': {'strengths': ['See technique documentation'], 'limitations': ['See technique documentation']}, 'references': [{'citation': "Lowe, D. G. 2004. 'Distinctive Image Features from Scale-Invariant Keypoints.' International Journal of Computer Vision 60 (2): 91-110.", 'doi': '10.1023/B:VISI.0000029664.99615.94'}, {'citation': "Hartley, R., and A. Zisserman. 2004. 'Multiple View Geometry in Computer Vision.' 2nd ed. Cambridge University Press.", 'doi': '10.1017/CBO9780511811685'}, {'citation': "Ubelmann, Y., and M. Hassan. 2017. 'Digitizing Palmyra: Photogrammetric Documentation Before and After Destruction.' Journal of Cultural Heritage 28: 162-170.", 'doi': '10.1016/j.culher.2017.05.011'}], 'lab_checklist': ['All images sharp (no motion blur, good focus)', 'Consistent exposure and white balance across image set', 'Sufficient overlap (70%+ between adjacent images)', 'All surfaces visible from at least 2 viewing angles', 'Camera alignment successful (all images registered)', 'Reprojection error <1 pixel (RMS; indicates good geometric consistency)', 'Dense point cloud complete (no large gaps or missing regions)', 'Model scaled correctly (measurements match ground truth within tolerance)', 'Texture well-aligned (no seams or color banding)', 'Common artifacts: Doming effect - curved surface appears on flat object due to insufficient camera network geometry (parallel camera positions), Bowling effect - similar to doming, systematic depth error from poor geometry, Noise and outliers - spurious points away from surface (from matching errors, reflections, moving objects), Holes and gaps - missing geometry due to occlusion, specular highlights, or low texture, Texture seams - visible boundaries where different photos meet on mesh (from lighting/exposure differences), Scale error - model dimensions incorrect (scale bar not measured or used incorrectly), Misalignments - chunks of model not properly registered (insufficient overlap or dynamic scene)', 'Troubleshooting: Some images not aligned (excluded from reconstruction) (check image quality (sharp, well-exposed); ensure overlap >60%; add more images to connect disconnected chunks; disable problematic images and re-align), High reprojection error (>1-2 pixels RMS) (use sharper images; calibrate camera properly; use global shutter camera if available; optimize camera parameters in bundle adjustment; remove outlier points), Large gaps or holes in reconstruction (capture additional views to see occluded areas; cross-polarize to reduce reflections; add temporary texture (careful: removable); accept limitation (some surfaces inherently difficult)), Doming or bowling (systematic depth error) (increase baseline diversity: capture from varying distances, heights, angles; add oblique and orthogonal views; avoid perfectly circular capture pattern; use more convergent geometry), Noisy point cloud (many spurious outliers) (improve image quality (sharper, lower ISO); capture under consistent lighting; remove moving objects from scene; apply noise filtering (statistical outlier removal, radius filtering) in post-processing), Incorrect scale (model too large or small) (re-measure scale bar carefully; verify markers clearly identified; use multiple scale bars for redundancy; cross-check dimensions against known measurements)'], 'keywords': ['photogrammetry', 'SfM', 'structure-from-motion', 'MVS', '3D reconstruction', '3D documentation', 'cultural heritage', 'digital heritage', 'archaeology', 'conservation']}
